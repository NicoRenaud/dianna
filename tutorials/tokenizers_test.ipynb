{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/aronjansen/Dropbox/eScience/projects/dianna-project/dianna/tutorials/tokenizers_test.ipynb Cell 1'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/aronjansen/Dropbox/eScience/projects/dianna-project/dianna/tutorials/tokenizers_test.ipynb#ch0000000?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m AutoTokenizer, AutoModelForSequenceClassification\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/aronjansen/Dropbox/eScience/projects/dianna-project/dianna/tutorials/tokenizers_test.ipynb#ch0000000?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdianna\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtokenizers\u001b[39;00m \u001b[39mimport\u001b[39;00m SpacyTokenizer\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from dianna.utils.tokenizers import SpacyTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 different tokenizers:\n",
    "- huggingface for language\n",
    "- huggingface for chemistry\n",
    "- custom, word based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AutoTokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/aronjansen/Dropbox/eScience/projects/dianna-project/dianna/tutorials/tokenizers_test.ipynb Cell 3'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/aronjansen/Dropbox/eScience/projects/dianna-project/dianna/tutorials/tokenizers_test.ipynb#ch0000002?line=0'>1</a>\u001b[0m model_name_language \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnlptown/bert-base-multilingual-uncased-sentiment\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/aronjansen/Dropbox/eScience/projects/dianna-project/dianna/tutorials/tokenizers_test.ipynb#ch0000002?line=1'>2</a>\u001b[0m model_name_chemistry \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mDeepChem/ChemBERTa-77M-MLM\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/aronjansen/Dropbox/eScience/projects/dianna-project/dianna/tutorials/tokenizers_test.ipynb#ch0000002?line=2'>3</a>\u001b[0m tokenizer_language \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(model_name_language)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/aronjansen/Dropbox/eScience/projects/dianna-project/dianna/tutorials/tokenizers_test.ipynb#ch0000002?line=3'>4</a>\u001b[0m tokenizer_chemistry \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(model_name_chemistry)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/aronjansen/Dropbox/eScience/projects/dianna-project/dianna/tutorials/tokenizers_test.ipynb#ch0000002?line=4'>5</a>\u001b[0m tokenizer_default \u001b[39m=\u001b[39m SpacyTokenizer(name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39men_core_web_sm\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'AutoTokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "model_name_language = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "model_name_chemistry = \"DeepChem/ChemBERTa-77M-MLM\"\n",
    "tokenizer_language = AutoTokenizer.from_pretrained(model_name_language)\n",
    "tokenizer_chemistry = AutoTokenizer.from_pretrained(model_name_chemistry)\n",
    "tokenizer_default = SpacyTokenizer(name='en_core_web_sm')\n",
    "\n",
    "model_language = AutoModelForSequenceClassification.from_pretrained(model_name_language)\n",
    "model_chemistry = AutoModelForSequenceClassification.from_pretrained(model_name_chemistry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'movie', 'is', 'shi', '##t', '.']\n",
      "['This', 'movie', 'is', 'shit']\n",
      "['C', 'C', '(', 'C', ')', 'C', 'C', '(', '=', 'O', ')']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"This movie is shit.\"\n",
    "molecule = \"CC(C)CC(=O)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'movie', 'is', 'shi', '##t', '.']\n",
      "['This', 'movie', 'is', 'shit']\n",
      "['C', 'C', '(', 'C', ')', 'C', 'C', '(', '=', 'O', ')']\n"
     ]
    }
   ],
   "source": [
    "tokens_language = tokenizer_language.tokenize(sentence)\n",
    "tokens_default = tokenizer_default.tokenize(sentence)\n",
    "tokens_chemistry = tokenizer_chemistry.tokenize(molecule)\n",
    "print(tokens_language)\n",
    "print(tokens_default)\n",
    "print(tokens_chemistry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# De-tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this movie is shit.\n",
      "This movie is shit\n",
      "CC(C)CC(=O)\n"
     ]
    }
   ],
   "source": [
    "sentence_hf = tokenizer_language.convert_tokens_to_string(tokens_language)\n",
    "sentence_custom = tokenizer_default.convert_tokens_to_string(tokens_default)\n",
    "molecule_decoded = tokenizer_chemistry.convert_tokens_to_string(tokens_chemistry)\n",
    "print(sentence_hf)\n",
    "print(sentence_custom)\n",
    "print(molecule_decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'movie', 'is', '[MASK]', '##t', '.']\n",
      "['This', 'movie', 'is', 'UNKWORDZ']\n",
      "['C', 'C', '(', '[MASK]', ')', 'C', 'C', '(', '=', 'O', ')']\n"
     ]
    }
   ],
   "source": [
    "masked_language = tokens_language[:]\n",
    "masked_language[3] = tokenizer_language.mask_token\n",
    "masked_default = tokens_default[:]\n",
    "masked_default[3] = tokenizer_default.mask_token\n",
    "masked_chemistry = tokens_chemistry[:]\n",
    "masked_chemistry[3] = tokenizer_chemistry.mask_token\n",
    "\n",
    "print(masked_language)\n",
    "print(masked_default)\n",
    "print(masked_chemistry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check that de-tokenizing and tokenizing the masked strings is the identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'movie', 'is', '[MASK]', '##t', '.'] ['this', 'movie', 'is', '[MASK]', 't', '.'] False\n",
      "['This', 'movie', 'is', 'UNKWORDZ'] ['This', 'movie', 'is', 'UNKWORDZ'] True\n",
      "['C', 'C', '(', '[MASK]', ')', 'C', 'C', '(', '=', 'O', ')'] ['C', 'C', '(', '[MASK]', ')', 'C', 'C', '(', '=', 'O', ')'] True\n"
     ]
    }
   ],
   "source": [
    "masked_decoded_language = tokenizer_language.tokenize(tokenizer_language.convert_tokens_to_string(masked_language))\n",
    "print(masked_language, masked_decoded_language, masked_language == masked_decoded_language) \n",
    "masked_decoded_default = tokenizer_default.tokenize(tokenizer_default.convert_tokens_to_string(masked_default))\n",
    "print(masked_default, masked_decoded_default, masked_default == masked_decoded_default) \n",
    "masked_decoded_chemistry = tokenizer_chemistry.tokenize(tokenizer_chemistry.convert_tokens_to_string(masked_chemistry))\n",
    "print(masked_chemistry, masked_decoded_chemistry, masked_chemistry == masked_decoded_chemistry) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this movie is [MASK]t.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_language.convert_tokens_to_string(masked_language)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE: it is NOT for the huggingface language tokenizer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "46ac0e5d3095c4497c061181ea9054f5ea4f47ef5144a77a8361c63aca65eec5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('dianna')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
