{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/dianna/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from dianna.utils.tokenizers import WordBasedTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 different tokenizers:\n",
    "- huggingface for language\n",
    "- huggingface for chemistry\n",
    "- custom, word based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at DeepChem/ChemBERTa-77M-MLM were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.decoder.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DeepChem/ChemBERTa-77M-MLM and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name_language = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "model_name_chemistry = \"DeepChem/ChemBERTa-77M-MLM\"\n",
    "tokenizer_language = AutoTokenizer.from_pretrained(model_name_language)\n",
    "tokenizer_chemistry = AutoTokenizer.from_pretrained(model_name_chemistry)\n",
    "tokenizer_default = WordBasedTokenizer()\n",
    "\n",
    "model_language = AutoModelForSequenceClassification.from_pretrained(model_name_language)\n",
    "model_chemistry = AutoModelForSequenceClassification.from_pretrained(model_name_chemistry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'movie', 'is', 'shi', '##t', '.']\n",
      "['This', 'movie', 'is', 'shit']\n",
      "['C', 'C', '(', 'C', ')', 'C', 'C', '(', '=', 'O', ')']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"This movie is shit.\"\n",
    "molecule = \"CC(C)CC(=O)\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'movie', 'is', 'shi', '##t', '.']\n",
      "['This', 'movie', 'is', 'shit']\n",
      "['C', 'C', '(', 'C', ')', 'C', 'C', '(', '=', 'O', ')']\n"
     ]
    }
   ],
   "source": [
    "tokens_language = tokenizer_language.tokenize(sentence)\n",
    "tokens_default = tokenizer_default.tokenize(sentence)\n",
    "tokens_chemistry = tokenizer_chemistry.tokenize(molecule)\n",
    "print(tokens_language)\n",
    "print(tokens_default)\n",
    "print(tokens_chemistry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# De-tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this movie is shit.\n",
      "This movie is shit\n",
      "CC(C)CC(=O)\n"
     ]
    }
   ],
   "source": [
    "sentence_hf = tokenizer_language.convert_tokens_to_string(tokens_language)\n",
    "sentence_custom = tokenizer_default.convert_tokens_to_string(tokens_default)\n",
    "molecule_decoded = tokenizer_chemistry.convert_tokens_to_string(tokens_chemistry)\n",
    "print(sentence_hf)\n",
    "print(sentence_custom)\n",
    "print(molecule_decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'movie', 'is', '[MASK]', '##t', '.']\n",
      "['This', 'movie', 'is', 'UNKWORDZ']\n",
      "['C', 'C', '(', '[MASK]', ')', 'C', 'C', '(', '=', 'O', ')']\n"
     ]
    }
   ],
   "source": [
    "masked_language = tokens_language[:]\n",
    "masked_language[3] = tokenizer_language.mask_token\n",
    "masked_default = tokens_default[:]\n",
    "masked_default[3] = tokenizer_default.mask_token\n",
    "masked_chemistry = tokens_chemistry[:]\n",
    "masked_chemistry[3] = tokenizer_chemistry.mask_token\n",
    "\n",
    "print(masked_language)\n",
    "print(masked_default)\n",
    "print(masked_chemistry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check that de-tokenizing and tokenizing the masked strings is the identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'movie', 'is', '[MASK]', '##t', '.'] ['this', 'movie', 'is', '[MASK]', 't', '.'] False\n",
      "['This', 'movie', 'is', 'UNKWORDZ'] ['This', 'movie', 'is', 'UNKWORDZ'] True\n",
      "['C', 'C', '(', '[MASK]', ')', 'C', 'C', '(', '=', 'O', ')'] ['C', 'C', '(', '[MASK]', ')', 'C', 'C', '(', '=', 'O', ')'] True\n"
     ]
    }
   ],
   "source": [
    "masked_decoded_language = tokenizer_language.tokenize(tokenizer_language.convert_tokens_to_string(masked_language))\n",
    "print(masked_language, masked_decoded_language, masked_language == masked_decoded_language) \n",
    "masked_decoded_default = tokenizer_default.tokenize(tokenizer_default.convert_tokens_to_string(masked_default))\n",
    "print(masked_default, masked_decoded_default, masked_default == masked_decoded_default) \n",
    "masked_decoded_chemistry = tokenizer_chemistry.tokenize(tokenizer_chemistry.convert_tokens_to_string(masked_chemistry))\n",
    "print(masked_chemistry, masked_decoded_chemistry, masked_chemistry == masked_decoded_chemistry) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this movie is [MASK]t.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_language.convert_tokens_to_string(masked_language)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE: it is NOT for the huggingface language tokenizer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "46ac0e5d3095c4497c061181ea9054f5ea4f47ef5144a77a8361c63aca65eec5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('dianna')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
